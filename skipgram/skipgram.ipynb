{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679dace6-1abe-4bce-96fa-df8dcc40ca8c",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a6c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "text = \"I like playing football because playing football is fun.\"\n",
    "\n",
    "tokens = text.lower().split()\n",
    "vocab = set(tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {j:i for i, j in enumerate(vocab)}\n",
    "idx_to_word = {j:i for i, j in word_to_idx.items()}\n",
    "\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f355d582-d6b4-4d96-b570-e2e75644320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_data(tokens, window_size):\n",
    "    data = []\n",
    "    for i in range(window_size, len(vocab) - window_size):\n",
    "        target = tokens[i]\n",
    "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
    "        for word in context:\n",
    "            data.append((target, word))\n",
    "    return data\n",
    "\n",
    "skipgram_data = create_skipgram_data(tokens, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5a9872-eb32-4fb7-b12d-3842203b5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_word):\n",
    "        target_vector = self.embeddings(target_word)\n",
    "        output = self.linear(target_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b17261-ba9c-4f2e-804d-2525d826481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "skipgram_model = SkipGram(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(skipgram_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229593e1-8e0d-4ec4-9125-8c9f83377fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 13.2314\n",
      "Epoch 20, Loss: 13.1655\n",
      "Epoch 30, Loss: 13.1361\n",
      "Epoch 40, Loss: 13.1166\n",
      "Epoch 50, Loss: 13.1012\n",
      "Epoch 60, Loss: 13.0878\n",
      "Epoch 70, Loss: 13.0758\n",
      "Epoch 80, Loss: 13.0646\n",
      "Epoch 90, Loss: 13.0542\n",
      "Epoch 100, Loss: 13.0443\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target, context in skipgram_data:\n",
    "        target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "        context_idx = torch.tensor([word_to_idx[context]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = skipgram_model(target_idx)\n",
    "        loss = criterion(output, context_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c61d94-605d-42b4-bb14-be114f3607dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1812e-01,  4.8279e-01,  5.5118e-01, -1.7796e-01,  2.5179e+00,\n",
       "          2.5021e-01,  7.2867e-01, -5.7593e-01,  7.7676e-01, -1.3737e+00,\n",
       "         -3.2198e-01, -7.6998e-01, -5.0072e-01,  1.2330e-01,  4.0705e-01,\n",
       "         -8.6523e-02,  7.5898e-01,  9.7002e-02, -1.0346e+00, -1.1127e-01,\n",
       "         -1.8332e-01,  9.8378e-01,  1.6859e-01,  4.4303e-01,  2.2903e-01,\n",
       "         -1.5132e-01,  2.2155e-01, -6.8202e-01, -3.8771e-01, -1.2499e+00,\n",
       "          1.5576e+00, -6.7147e-02, -4.1470e-01, -8.1196e-01,  2.8030e-01,\n",
       "          9.7454e-01,  6.3801e-01,  9.2477e-03, -3.0372e-01,  7.9786e-01,\n",
       "         -1.1112e+00, -1.7083e+00,  1.2825e+00,  6.1507e-02, -3.0835e-01,\n",
       "          3.8652e-01,  3.9943e-06,  1.6661e+00,  2.0936e-01, -2.2449e-01],\n",
       "        [ 6.9256e-01, -1.2926e-01, -2.0364e+00, -4.7599e-01, -7.2472e-01,\n",
       "         -8.3629e-01,  5.8204e-01, -1.4702e+00,  2.4645e-03, -3.7806e-01,\n",
       "          8.5085e-01,  7.4890e-01, -7.1798e-01,  1.6889e+00,  3.9534e+00,\n",
       "          2.6306e-01, -4.6721e-01,  4.2503e-01,  2.2103e-01, -9.1069e-01,\n",
       "          3.3941e-01, -2.6684e-01,  2.6638e+00,  2.4769e-01,  6.4721e-01,\n",
       "         -6.8055e-01, -1.3965e+00, -1.1866e+00,  6.6327e-01, -7.6034e-01,\n",
       "         -5.3388e-01, -2.7068e-01,  3.1762e-01,  6.5339e-02,  7.3419e-01,\n",
       "         -6.9617e-02,  1.3136e+00, -4.1568e-01, -3.8033e-01, -3.3842e-01,\n",
       "          6.9375e-01,  9.9858e-01, -5.1100e-01,  1.1237e+00,  3.9552e-01,\n",
       "          9.0480e-02,  7.5597e-01, -1.3917e-01,  1.1533e+00,  1.4243e+00],\n",
       "        [-1.4311e+00, -3.4968e-01,  6.1964e-02, -4.2671e-01,  2.1281e-01,\n",
       "         -3.1402e-01, -7.7972e-03,  4.5640e-01, -1.0332e+00,  2.1007e+00,\n",
       "         -4.7744e-01,  7.2466e-01,  3.6432e-01, -1.2528e+00,  2.0507e+00,\n",
       "         -1.0490e+00, -1.4283e-02,  1.8038e+00, -7.1949e-01,  8.5247e-01,\n",
       "          6.8280e-03,  3.9433e-01, -1.2497e+00, -4.6958e-01,  1.5460e+00,\n",
       "         -5.5653e-01,  2.2037e-01,  8.8614e-01, -3.8922e-01, -6.2681e-01,\n",
       "          2.7264e-01, -3.7683e-01,  5.1397e-01,  8.2298e-01,  5.1983e-01,\n",
       "         -1.1728e+00, -5.5190e-01, -9.4725e-02, -6.6853e-02,  9.1821e-01,\n",
       "          4.5552e-01, -1.9436e-01,  3.1635e-01, -2.4426e-01, -4.1958e-01,\n",
       "          5.3990e-01, -1.9970e+00, -1.4227e+00,  5.6267e-01,  1.3723e-01],\n",
       "        [ 3.5612e-01,  9.0892e-01, -9.3544e-01,  4.0713e-01, -1.0505e+00,\n",
       "         -1.6617e-01,  1.1261e+00, -2.3833e-01, -1.0370e-02, -2.2776e+00,\n",
       "         -9.4688e-01,  3.0921e+00,  1.0643e+00, -5.8439e-01, -1.0739e+00,\n",
       "         -1.5373e+00, -1.3442e-01, -8.9965e-01, -9.3102e-01,  9.5993e-01,\n",
       "         -8.0565e-01,  2.4230e-01,  1.2473e+00,  9.3464e-02,  5.9503e-01,\n",
       "          4.2735e-01,  1.5761e+00,  4.9467e-01, -1.3420e+00, -1.3651e-01,\n",
       "          1.4238e+00, -8.6454e-01, -7.7029e-02, -8.4396e-01, -8.6654e-01,\n",
       "         -1.2866e+00,  1.0834e+00, -9.3163e-01, -1.3732e-01,  1.3759e-01,\n",
       "         -6.0273e-02, -1.1752e+00, -3.0871e+00,  1.6685e+00,  5.5972e-01,\n",
       "         -2.1634e+00,  1.4869e+00,  1.5302e+00, -5.6745e-01, -1.2975e+00],\n",
       "        [-2.7611e-01,  2.3624e+00,  6.3730e-01,  1.7883e+00, -6.0830e-01,\n",
       "          7.9652e-01,  7.0726e-01,  1.5507e+00, -1.6307e+00,  1.1487e+00,\n",
       "         -3.1948e-01, -5.0114e-01, -5.7566e-01, -5.3229e-01,  6.0046e-01,\n",
       "          4.9717e-01,  3.6424e-01, -1.6170e-01,  1.1806e+00, -1.8029e-01,\n",
       "          3.9905e-01, -1.1837e+00,  7.0768e-01,  3.5800e-01, -2.7390e-01,\n",
       "          3.0189e-01, -5.3500e-01,  8.6249e-02, -1.4934e+00,  3.2373e-01,\n",
       "          7.0138e-01, -4.1536e-01,  8.5762e-01,  2.2105e-01, -1.5773e+00,\n",
       "          6.0955e-01,  2.7489e-01, -8.0108e-01,  2.2704e-01,  1.5946e+00,\n",
       "         -7.1006e-01,  1.0716e-01, -3.7822e-02,  8.6629e-01, -8.8722e-01,\n",
       "          4.1220e-01,  9.9413e-01,  7.0530e-01,  1.5883e+00,  7.6793e-01],\n",
       "        [ 1.2059e+00, -4.6687e-01,  2.0479e-01,  9.9844e-01, -6.8783e-01,\n",
       "          1.5630e+00, -7.5041e-01, -1.4466e+00,  7.4632e-01, -1.2983e+00,\n",
       "         -1.0997e+00,  9.5847e-01, -2.2426e-01, -1.6207e-01,  1.4192e+00,\n",
       "         -6.5451e-01, -1.6252e-01,  1.5298e+00,  9.1821e-01, -5.0969e-02,\n",
       "         -9.8022e-01,  5.7134e-02, -1.4851e-01,  9.3583e-01,  8.9737e-01,\n",
       "          1.8019e+00,  9.4812e-01,  3.4868e-01,  5.9440e-02,  7.6145e-02,\n",
       "          3.3781e-01, -1.6605e+00,  9.3444e-01, -4.2570e-01,  3.9047e-01,\n",
       "         -2.3601e-03, -1.5320e+00, -1.4783e+00,  3.9562e-02,  5.6889e-01,\n",
       "          1.0123e-01,  9.0879e-01, -3.7103e-01, -3.3207e+00, -3.5999e-02,\n",
       "          1.1566e+00, -9.2037e-01, -7.3598e-01,  6.1100e-01, -2.7354e-01],\n",
       "        [ 1.9224e-01, -1.2490e+00, -1.5327e+00,  6.9111e-01,  1.9195e+00,\n",
       "          1.3692e+00, -3.2052e-01,  1.9483e+00, -5.4512e-01,  1.2557e+00,\n",
       "         -1.4097e+00, -1.2043e+00, -8.4651e-01, -1.2384e+00, -1.3997e+00,\n",
       "          2.3260e-01,  6.9750e-01, -3.3037e-01, -1.9366e+00, -5.0390e-01,\n",
       "         -3.7110e-01,  1.4274e+00, -1.6221e+00,  4.1606e-01, -4.0345e-01,\n",
       "          1.9390e-01, -7.5176e-01,  2.3869e+00,  8.5310e-01,  5.8885e-02,\n",
       "         -5.6826e-01, -2.9614e-01, -5.5348e-01, -8.2888e-01, -1.6742e-01,\n",
       "          2.0488e+00, -3.3334e-02, -7.1877e-01, -1.6182e+00,  6.7065e-01,\n",
       "          4.5117e-01,  7.6406e-02, -7.1780e-02, -2.2813e-01,  1.5516e+00,\n",
       "          1.4373e+00, -1.8115e+00,  9.3800e-01, -1.1061e+00,  9.1810e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338a393f-ae97-4d47-98eb-e16da9bb8296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Word: playing, Predicted Context Words: ['because', 'football', 'like']\n"
     ]
    }
   ],
   "source": [
    "def predict_skipgram(target, model, word_to_idx, idx_to_word, top_n=5):\n",
    "    target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        output = model(target_idx)\n",
    "        predicted_probs = torch.softmax(output, dim=1)\n",
    "        top_context_idxs = torch.topk(predicted_probs, top_n, dim=1).indices.squeeze(0).tolist()\n",
    "    return [idx_to_word[idx] for idx in top_context_idxs]\n",
    "\n",
    "# Test Skip-gram prediction\n",
    "target_word = \"playing\"\n",
    "predicted_context = predict_skipgram(target_word, skipgram_model, word_to_idx, idx_to_word, top_n=3)\n",
    "print(f\"Target Word: {target_word}, Predicted Context Words: {predicted_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b23ef2c-f04e-4f24-9bc5-f19b64600904",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = skipgram_model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4487b-ad36-4969-978a-4e2e8aeab85c",
   "metadata": {},
   "source": [
    "##  Finding Similar Words\n",
    "* Use cosine similarity to find the most similar words in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c4594d-d0bd-4756-92e3-3c2f88190c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'football': [('fun.', 0.18023665558995083), ('because', 0.10099966195637167), ('i', -6.411839770459338e-05), ('like', -0.030076215151108254), ('playing', -0.10613631192153772)]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_similar(word, word_embeddings, word_to_idx, idx_to_word, top_n=5):\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_vector = word_embeddings[word_idx]\n",
    "    similarities = []\n",
    "    for idx, vec in enumerate(word_embeddings):\n",
    "        if idx != word_idx:\n",
    "            similarity = 1 - cosine(word_vector, vec)\n",
    "            similarities.append((similarity, idx))\n",
    "    top_similar = sorted(similarities, key=lambda x: x[0], reverse=True)[:top_n]\n",
    "    return [(idx_to_word[idx], sim) for sim, idx in top_similar]\n",
    "\n",
    "# Test finding similar words\n",
    "similar_words = find_similar(\"football\", word_embeddings, word_to_idx, idx_to_word)\n",
    "print(f\"Words similar to 'football': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c660d2-35a8-4f07-8791-c58194614f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bac85-d06e-4f75-9311-f09fcd65b093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96819ec4-9cf7-4419-8c06-3ccecfa70d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
