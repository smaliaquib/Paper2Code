{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f329823a-e74f-43fb-95ad-c86a2c9f2c29",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19900a4f-5347-4c67-802f-9d3ff80e821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "text = \"I like playing football because playing football is fun.\"\n",
    "\n",
    "# Tokenize and preprocess\n",
    "tokens = text.lower().split()\n",
    "vocab = set(tokens)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create word-to-index and index-to-word mappings\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "# Define window size for context\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1ff42b-d9c9-44da-be50-86fc3c62c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW data preparation\n",
    "def create_cbow_data(tokens, window_size):\n",
    "    data = []\n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
    "        target = tokens[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "cbow_data = create_cbow_data(tokens, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8569a12-c58b-4d5d-8655-630290ecd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        # Get embeddings for context words and average them\n",
    "        context_vectors = self.embeddings(context_words)\n",
    "        context_mean = context_vectors.mean(dim=1)\n",
    "        output = self.linear(context_mean)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c35cef-0f25-4850-80d0-8e39ba538617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.2272\n",
      "Epoch 20, Loss: 0.0696\n",
      "Epoch 30, Loss: 0.0374\n",
      "Epoch 40, Loss: 0.0237\n",
      "Epoch 50, Loss: 0.0165\n",
      "Epoch 60, Loss: 0.0122\n",
      "Epoch 70, Loss: 0.0094\n",
      "Epoch 80, Loss: 0.0075\n",
      "Epoch 90, Loss: 0.0061\n",
      "Epoch 100, Loss: 0.0051\n"
     ]
    }
   ],
   "source": [
    "# Training CBOW model\n",
    "embedding_dim = 50\n",
    "cbow_model = CBOW(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in cbow_data:\n",
    "        context_idx = torch.tensor([word_to_idx[word] for word in context], dtype=torch.long).unsqueeze(0)\n",
    "        target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(context_idx)\n",
    "        loss = criterion(output, target_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e20c446-a557-4c98-bd1a-58b24097cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 29.0170\n",
      "Epoch 20, Loss: 28.7953\n",
      "Epoch 30, Loss: 28.6348\n",
      "Epoch 40, Loss: 28.5015\n",
      "Epoch 50, Loss: 28.3869\n",
      "Epoch 60, Loss: 28.2869\n",
      "Epoch 70, Loss: 28.1987\n",
      "Epoch 80, Loss: 28.1206\n",
      "Epoch 90, Loss: 28.0510\n",
      "Epoch 100, Loss: 27.9887\n"
     ]
    }
   ],
   "source": [
    "# Training Skip-gram model\n",
    "embedding_dim = 50\n",
    "skipgram_model = SkipGram(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target, context in skipgram_data:\n",
    "        target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "        context_idx = torch.tensor([word_to_idx[context]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = skipgram_model(target_idx)\n",
    "        loss = criterion(output, context_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d633570-89b1-4f69-8b9a-df2aa66759ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0180,  0.3982, -0.4570,  0.9488,  3.2252,  0.0494,  0.2571, -1.4554,\n",
       "          1.9020,  1.1556, -0.1488,  0.4014,  1.3506,  1.9969,  0.4816, -1.1947,\n",
       "         -1.3340, -0.0808, -0.7911, -0.6684,  0.5220,  1.8570, -0.5459,  0.7342,\n",
       "          0.9366, -1.3944, -1.4262, -0.7894, -1.3418, -0.1346, -1.8685,  1.4023,\n",
       "          1.1101,  1.0732,  3.2738,  1.4829,  0.1860, -0.1798,  0.5820, -1.0874,\n",
       "          0.9004,  1.0829, -0.9059,  0.1669, -1.0915, -0.0370,  1.1727, -0.2422,\n",
       "          1.2343,  1.2383],\n",
       "        [-0.2533,  0.6273,  1.0680, -0.1701,  1.1222, -1.4697,  1.8079, -1.1539,\n",
       "          0.1433,  0.3029,  0.2832,  2.0881,  1.3713, -2.1716,  1.5298,  1.3365,\n",
       "          1.9066,  0.0168,  1.4105, -0.6676, -0.9336,  1.0564, -2.1765,  2.1596,\n",
       "         -0.6975, -0.0257, -1.2667, -0.8645,  0.5362,  0.2247,  0.1601,  0.3255,\n",
       "          0.0267,  1.8353, -2.2286,  1.4602,  1.5984,  1.0063, -0.4191,  0.9502,\n",
       "          0.3253, -0.6747, -0.8185, -1.5516, -1.5385,  0.5184, -0.7134,  0.4234,\n",
       "          0.3670,  1.7801],\n",
       "        [-0.7029, -1.2998,  1.2689,  1.5594, -0.2345,  1.2144,  0.2414, -0.5793,\n",
       "          0.5363, -0.5055, -0.2030, -1.7857,  0.4660, -2.4903,  0.2732, -0.9423,\n",
       "         -1.3542,  2.0300,  1.8451, -1.1893,  0.2545, -1.0618,  1.0641, -0.1349,\n",
       "          1.3129,  1.0466,  0.4315,  0.0766,  0.8682, -0.2441,  1.4273, -3.0052,\n",
       "         -0.2224, -2.3275,  0.9200, -0.2611,  0.9100,  0.9801, -0.4685, -2.7961,\n",
       "         -1.5884,  0.8212, -1.1890, -1.0879,  0.2835,  1.5529,  1.0122, -2.1198,\n",
       "         -0.6935, -0.6441],\n",
       "        [-0.9508,  1.3293, -0.3447, -0.0847, -0.5432, -1.8053, -1.9437,  1.2050,\n",
       "          0.5204,  0.8189,  1.1006,  1.3972, -0.1416, -0.2905,  1.1871, -1.5488,\n",
       "          0.0503, -1.1109, -0.2566, -0.8328, -0.8899,  0.0598, -0.1308, -0.8160,\n",
       "         -0.0654, -1.7623,  3.0056, -2.1271, -0.9780,  0.5345, -2.3236,  1.7364,\n",
       "          1.6827, -0.3600,  1.5512,  0.8509, -1.0319, -1.6120,  0.6269, -0.0974,\n",
       "         -0.1108, -0.3466,  0.6783,  1.2299, -0.5322, -1.8095,  0.8609, -1.6153,\n",
       "          0.0751,  0.4122],\n",
       "        [-0.4875, -1.8694, -0.0135,  0.5376, -0.6069,  2.6863,  0.0324, -0.7844,\n",
       "          0.2777,  0.3700,  1.1006, -1.0645,  0.7778, -0.1098, -0.5195,  1.3564,\n",
       "          0.8113,  1.0401, -0.4072,  1.0763, -0.4917, -1.0899, -2.1390,  1.1049,\n",
       "         -1.7229,  0.6359, -0.6202, -0.3180,  1.0539, -0.1391, -1.0691,  1.7844,\n",
       "         -0.6781,  0.7581,  0.0084,  1.0845,  0.2202,  0.3954, -1.6111, -1.0548,\n",
       "         -0.9145,  1.3706, -0.6504, -0.2521, -0.9642,  1.3068, -0.3708,  0.0707,\n",
       "          0.6472, -1.1339],\n",
       "        [ 0.1716, -0.5029,  2.2274, -0.8778,  0.6715,  0.1337,  2.1035, -0.4668,\n",
       "          0.3903, -0.5060,  0.4281, -0.1467,  0.1172,  1.5093, -0.4170,  0.2519,\n",
       "          1.2104, -1.3513,  1.6260, -1.3107,  0.1379, -0.5895, -2.8812,  1.4426,\n",
       "          0.6250, -1.3621,  0.1148,  0.1939,  0.2838, -0.5307,  0.8486, -1.8089,\n",
       "          1.8303,  0.9346, -0.2100, -0.7066, -0.5172,  0.6810,  0.2294, -2.1069,\n",
       "          1.3533,  0.5118, -0.4513, -0.4647, -0.8377,  0.4681, -2.4033, -0.4276,\n",
       "          0.2496,  2.5080],\n",
       "        [-1.5024,  0.7656, -0.7916,  1.0198,  1.1431,  1.1049, -0.9768, -1.5257,\n",
       "         -0.0686,  1.4752,  1.5251, -1.9145,  1.6419,  1.4627, -0.2580,  0.1107,\n",
       "         -0.2212,  0.3821, -0.7314, -1.3480, -0.1267,  0.6364,  0.0505,  0.5525,\n",
       "         -0.2921, -0.7259, -0.8353, -1.3343,  0.6558,  1.9016,  0.7911, -0.0356,\n",
       "          0.4259,  0.1956, -0.7550,  0.5812,  0.3338,  0.5656,  2.1677, -1.0776,\n",
       "          0.0682,  0.7171, -1.4923,  0.5301, -2.4804,  0.3375,  1.1590,  0.2348,\n",
       "          0.9783,  2.1922]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e5e0df-d2b7-4473-ba39-431b009e4000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['i', 'like', 'football', 'because'], Predicted Word: playing\n"
     ]
    }
   ],
   "source": [
    "def predict_cbow(context, model, word_to_idx, idx_to_word):\n",
    "    context_idx = torch.tensor([word_to_idx[word] for word in context], dtype=torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(context_idx)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    return idx_to_word[predicted_idx]\n",
    "\n",
    "# Test CBOW prediction\n",
    "context = [\"i\", \"like\", \"football\", \"because\"]\n",
    "predicted_word = predict_cbow(context, cbow_model, word_to_idx, idx_to_word)\n",
    "print(f\"Context: {context}, Predicted Word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5f19f0-ee13-45a4-b5c6-6089a8cae963",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = cbow_model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d203f-5a68-4792-b897-540a6563eb32",
   "metadata": {},
   "source": [
    "##  Finding Similar Words\n",
    "* Use cosine similarity to find the most similar words in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39f167-a582-4a3e-b922-a38a1e351e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_similar(word, word_embeddings, word_to_idx, idx_to_word, top_n=5):\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_vector = word_embeddings[word_idx]\n",
    "    similarities = []\n",
    "    for idx, vec in enumerate(word_embeddings):\n",
    "        if idx != word_idx:\n",
    "            similarity = 1 - cosine(word_vector, vec)\n",
    "            similarities.append((similarity, idx))\n",
    "    top_similar = sorted(similarities, key=lambda x: x[0], reverse=True)[:top_n]\n",
    "    return [(idx_to_word[idx], sim) for sim, idx in top_similar]\n",
    "\n",
    "# Test finding similar words\n",
    "similar_words = find_similar(\"football\", word_embeddings, word_to_idx, idx_to_word)\n",
    "print(f\"Words similar to 'football': {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5937c-ff2e-4996-b5d5-133f646b5142",
   "metadata": {},
   "source": [
    "## Word Analogies\n",
    "* Solve analogies like king - man + woman ≈ queen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd165bf-eef8-444d-a068-8a150d8e05c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'king'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m idx_to_word[top_match[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Test word analogy\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m word_analogy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, word_embeddings, word_to_idx, idx_to_word)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m + \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ≈ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mword_analogy\u001b[1;34m(word_a, word_b, word_c, word_embeddings, word_to_idx, idx_to_word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_analogy\u001b[39m(word_a, word_b, word_c, word_embeddings, word_to_idx, idx_to_word):\n\u001b[1;32m----> 2\u001b[0m     vec_a \u001b[38;5;241m=\u001b[39m word_embeddings[word_to_idx[word_a]]\n\u001b[0;32m      3\u001b[0m     vec_b \u001b[38;5;241m=\u001b[39m word_embeddings[word_to_idx[word_b]]\n\u001b[0;32m      4\u001b[0m     vec_c \u001b[38;5;241m=\u001b[39m word_embeddings[word_to_idx[word_c]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'king'"
     ]
    }
   ],
   "source": [
    "def word_analogy(word_a, word_b, word_c, word_embeddings, word_to_idx, idx_to_word):\n",
    "    vec_a = word_embeddings[word_to_idx[word_a]]\n",
    "    vec_b = word_embeddings[word_to_idx[word_b]]\n",
    "    vec_c = word_embeddings[word_to_idx[word_c]]\n",
    "    target_vector = vec_a - vec_b + vec_c\n",
    "    similarities = []\n",
    "    for idx, vec in enumerate(word_embeddings):\n",
    "        similarity = 1 - cosine(target_vector, vec)\n",
    "        similarities.append((similarity, idx))\n",
    "    # Exclude input words from the result\n",
    "    input_idxs = {word_to_idx[word_a], word_to_idx[word_b], word_to_idx[word_c]}\n",
    "    top_match = max((sim, idx) for sim, idx in similarities if idx not in input_idxs)\n",
    "    return idx_to_word[top_match[1]]\n",
    "\n",
    "# Test word analogy\n",
    "result = word_analogy(\"king\", \"man\", \"woman\", word_embeddings, word_to_idx, idx_to_word)\n",
    "print(f\"'king' - 'man' + 'woman' ≈ '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8c244-1382-4871-8349-9edd71b11dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aff20-b235-459c-bcc9-90cd8b65f4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb095f0-d45c-441f-9319-34878305fb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
