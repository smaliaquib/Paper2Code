{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2787a7c95d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * np.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Creating a temp (seq_len x embed_dim) matrix\n",
    "        pe = torch.zeros((seq_len, d_model))\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        ## Create 1D tensor by doing unsqueeze till range of sequence\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # (1, Seq_len, d_model)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (self.pe[:, :x.size(1), :]).requires_grad_(False) # (batch, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is String: my name is jarvis i am tony stark bot, my daily task is to assist iron man\n",
      "This is vocabulary: ['am', 'assist', 'bot,', 'daily', 'i', 'iron', 'is', 'jarvis', 'man', 'my', 'name', 'stark', 'task', 'to', 'tony']\n",
      "\n",
      "This is token to id mapping {'am': 0, 'assist': 1, 'bot,': 2, 'daily': 3, 'i': 4, 'iron': 5, 'is': 6, 'jarvis': 7, 'man': 8, 'my': 9, 'name': 10, 'stark': 11, 'task': 12, 'to': 13, 'tony': 14}\n",
      "Sentence to Token ID tensor([ 9, 10,  6,  7,  4,  0, 14, 11,  2,  9,  3, 12,  6, 13,  1,  5,  8])\n"
     ]
    }
   ],
   "source": [
    "st = \"my name is jarvis i am tony stark bot, my daily task is to assist iron man\"\n",
    "vocab = sorted(set(st.split()))\n",
    "\n",
    "print(f\"This is String: {st}\\nThis is vocabulary: {vocab}\")\n",
    "\n",
    "vocab_id = {j:i for i, j in enumerate(vocab)}\n",
    "print(f\"\\nThis is token to id mapping {vocab_id}\")\n",
    "\n",
    "tokens_ids = torch.tensor([vocab_id[i] for i in st.split()])\n",
    "print(f\"Sentence to Token ID {tokens_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "d_model = 100\n",
    "seq_len = 50\n",
    "\n",
    "embed_obj = InputEmbedding(d_model, len(vocab))\n",
    "embedding = embed_obj(tokens_ids)\n",
    "\n",
    "embedding = embedding.unsqueeze(0)\n",
    "\n",
    "pe_obj = PositionalEncoding(seq_len, d_model)\n",
    "encoded_position = pe_obj(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 100])\n",
      "torch.Size([17, 100])\n",
      "tensor([[-11.3080,  -0.4911,  14.4497,  ...,   5.8652,  -6.9199,  21.8012],\n",
      "        [ -5.2923,  -7.2075,  -1.4856,  ...,  -1.8305,  -6.6310,   1.0759],\n",
      "        [ 14.5444, -25.4209,  -6.8304,  ...,  10.9295,   9.0678,  12.9104],\n",
      "        ...,\n",
      "        [  1.0780,   7.1567,  -0.9054,  ...,  -3.5234,   8.0529,   4.1451],\n",
      "        [  4.7299,  -6.9776,  -9.9911,  ...,  10.8097,   4.8701,  19.3318],\n",
      "        [ -8.4247,  15.4856, -16.2390,  ..., -11.5524,  -4.5310,   2.2455]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_position.shape)\n",
    "\n",
    "encoded_position = encoded_position.squeeze(0)\n",
    "print(encoded_position.shape)\n",
    "print(encoded_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tony, and it Embedding is\n",
      " tensor([ -1.7055,  -2.0192,   0.6594, -14.1280,   2.3142,  -2.4818,   2.6023,\n",
      "         -7.8622,   0.8334,  -0.2319, -17.2297, -24.1096,   4.2452,   1.8019,\n",
      "         -8.3507,   8.8680, -12.4589,   5.3780,  -5.6493,  -7.6722, -10.2631,\n",
      "          6.1659,  -9.4200,   7.5622,  21.1879,  -8.5192,  -4.4970,  -0.4677,\n",
      "         -2.5026,   0.2160,  -8.7706,  17.6558,   8.7257,  14.5058, -17.2108,\n",
      "         11.1512,  11.6407,  -7.3563,  -2.3259,  -6.3369, -28.3395,  -3.1543,\n",
      "         18.6783, -12.3554,  -3.7060,   2.9567,  -1.5735,   6.0819,  -4.3824,\n",
      "          2.3967,  -6.7383,  -2.0250, -11.5031,  12.1552,  -9.3809,   5.1657,\n",
      "          3.2447,  -3.6813,  -2.2434,  -2.5731, -10.1957,   8.1627,  16.3385,\n",
      "          2.8388,   6.8943,   3.2725,  -1.5134,  -5.2289,   2.1171,  20.7504,\n",
      "         -5.7956,   5.2520,  11.2568,  16.6707,   5.1538,   2.5021,  -9.6319,\n",
      "         16.2356, -16.6142, -16.8092,   0.1687,   1.6515,   4.9326, -28.6015,\n",
      "         -2.5305,  -3.2146,   6.5224,  15.0104,   0.6200,   3.3775,   4.9467,\n",
      "         -1.3358,  -2.9801,   7.6646,   2.2694,  15.1755,   2.8118,   3.5007,\n",
      "          1.2960, -15.9981], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word tony, and it Embedding is\\n {encoded_position[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = encoded_position.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 12, 12, 20\n",
    "\n",
    "W_query = nn.Parameter(torch.randn(d_q, dim))\n",
    "W_key = nn.Parameter(torch.randn(d_k, dim))\n",
    "W_value = nn.Parameter(torch.randn(d_v, dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  15.6946,   21.9732,   16.1444,  104.4243,  238.8973, -138.9019,\n",
      "         -47.1492,  -97.1442,  -48.5859, -181.1216,   85.7165,   53.9765],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "## For tony its caculated\n",
    "print((W_key @ encoded_position.T).T[1])\n",
    "print((W_key @ encoded_position.T).T[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([17, 12])\n",
      "torch.Size([17, 20])\n",
      "tensor([ 21642.4121,  38417.5898,  11507.6641,   4144.8604, -23516.4512,\n",
      "         21029.3652,   7124.0415, -26574.6055, -11337.7773,  21508.5801,\n",
      "        -75335.9922,    805.1486,  12766.4014,  13720.6562,  -9677.3672,\n",
      "        -47455.4375,  29212.1992], grad_fn=<SqueezeBackward4>)\n",
      "Most similar word for tony is 2nd token with score of 38417.58984375\n"
     ]
    }
   ],
   "source": [
    "query_tony = W_query @ encoded_position[6]\n",
    "print(query_tony.shape)\n",
    "\n",
    "keys = (W_key @ encoded_position.T).T\n",
    "print(keys.shape)\n",
    "\n",
    "values = (W_value @ encoded_position.T).T\n",
    "print(values.shape)\n",
    "\n",
    "\n",
    "omega_tony = query_tony @ keys.T\n",
    "print(omega_tony)\n",
    "print(f\"Most similar word for tony is {torch.argmax(omega_tony) + 1}nd token with score of {max(omega_tony)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  84.8025,   39.4382,  -70.7152,  119.1056,  -70.6185, -202.8909,\n",
      "         -66.1431,  -12.9007,  106.1452,  161.7344,  -55.3745,  116.2459,\n",
      "        -211.0940,   55.9901,  -42.4252, -112.2250,   47.3509,  -94.3177,\n",
      "         -35.5337,  -47.9927], grad_fn=<SqueezeBackward4>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aquib\\AppData\\Local\\Temp\\ipykernel_14988\\2388748292.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  scaled_dot  = F.softmax(omega_tony / np.sqrt(d_k))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "scaled_dot  = F.softmax(omega_tony / np.sqrt(d_k)) \n",
    "\n",
    "# multiplying with values\n",
    "context_vector_tony = scaled_dot @ values\n",
    "print(context_vector_tony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads ==0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads ## d_k\n",
    "        # d_k = d_v = d_model // num_heads\n",
    "\n",
    "        ## Linear Projection for Q, K, V\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Output Layer\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # scaling\n",
    "        self.scale = np.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \n",
    "        batch = q.size(0)\n",
    "\n",
    "        query = self.linear_q(q)\n",
    "        key = self.linear_k(k)\n",
    "        value = self.linear_v(v)\n",
    "\n",
    "        # Split Q, K, V into multiple heads\n",
    "        query = query.view(batch, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = (query @ key.transpose(-1, -2)) / self.scale\n",
    "        # Scaled dot product attention\n",
    "        if mask is not None:\n",
    "            scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = self.softmax(scores)\n",
    "        attention_output = attention_weights @ value\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch, -1, self.num_heads * self.head_dim)\n",
    " \n",
    "        # Final linear transformation\n",
    "        output = self.out(attention_output)\n",
    " \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_position.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(100, 2)\n",
    "# d_model % 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6.5549,  0.4533,  5.9515,  ...,  1.3006,  1.3325,  0.3603]],\n",
       " \n",
       "         [[-0.1755, -0.4881, -0.5081,  ...,  3.2096, -0.6231, -2.4404]],\n",
       " \n",
       "         [[-3.5673, -3.7174, -0.2607,  ..., -1.1323, -0.7412,  0.6034]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.0202,  1.7797,  1.2958,  ..., -2.8006,  2.4137, -0.3167]],\n",
       " \n",
       "         [[-1.2335, -3.2440,  2.6548,  ..., -3.4210, -9.5924,  4.3435]],\n",
       " \n",
       "         [[ 1.0761, -6.5702, -1.9288,  ...,  1.4405,  2.5107, -3.0740]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]],\n",
       " \n",
       " \n",
       "         [[[1.]],\n",
       " \n",
       "          [[1.]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(encoded_position, encoded_position, encoded_position, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
